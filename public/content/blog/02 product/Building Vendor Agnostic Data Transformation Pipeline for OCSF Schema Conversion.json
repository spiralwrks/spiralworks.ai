{
  "title": "Building Vendor Agnostic Data Transformation Pipeline for OCSF Schema Conversion",
  "date": "2025-01-12T18:22:49.824Z",
  "tags": [],
  "content": "This is the core data engineering project for [[OCSF]] based log data. The goal is to make the data transformation pipeline vendor agnostic such that our model is able to train on any customer data and adapt accordingly. Initial research from studying the [OCSF examples repo](https://github.com/ocsf/examples) showed the following:\n\n1. There exists a guideline to [perform and submit mappings](https://github.com/ocsf/examples/tree/main?tab=readme-ov-file#mappings)\n2. Some pull requests consist of [extensive mappings from various vendors](https://github.com/ocsf/examples/pull/64/files/0497a8a3627ddf8f26f6c385ccfaa6a9865ce555). However, there are validation issues due to missing fields (assumption: the mappings failed to provide to the fields expected by the OCSF schema)\n3. [One PR](https://github.com/ocsf/examples/pull/74/files) is regarding inviting various vendors/users to donate the raw logs to facilitate OCSF mapping\n4. The [example mappings](https://github.com/ocsf/examples/tree/main/mappings/bloblang) include various data transformation architectures for AWS products (CloudTrail, EKS, Route53) where the raw logs are passed `blobl` files that convert raw logs into OCSF format (in purple.)\n   ![[Pasted image 20240729181732.png]]\n   `AWS CloudTrail raw log --> OCSF pipeline`\n   ![[Pasted image 20240729181950.png]]\n   `AWS Route53 raw log --> OCSF pipeline`\n   \n   Note: The first transformations are vendor and raw log specific. The second transformations are universal OCSF transformations. They are performed utilizing the `blobl` files.\n   Other ETL pipelines exist. E.g., [the `Data Prepper` framework](https://github.com/ocsf/examples/tree/main/mappings/dataprepper.) However, it is much lower level and requires manual manipulation of the fields in the `pipeline.yaml` file.\n   \nBased on the above insights, we would like to make the vendor and raw log specific first transformations more generalized. The goal is to be able to dynamically handle any type of incoming logs. To do this, we could do the following (hypothesis pipeline):\n\n1. Parse the raw logs handling any format (`xml`, `json`, etc.) for various vendors and log types (e.g., `account`, `API`, `authentication`, etc.)\n2. Dynamically generate `blobl` files according to the parsed information in step 1\n3. The rest of the pipeline remains relatively static.\n\nAs a result, we would like to utilize LLM agents such as [LangGraph](https://github.com/langchain-ai/langgraph) for steps 1. and 2. The schema validation can be done using `ocsf-py` programmatically.\n\n## The pipeline\n\nBased on the above insights, initial hypotheses, and the recent discussions more details of the pipeline emerged as follows: \n## A. ETL Step (OCSF Conversion) \n1. Generate and collect raw logs from various vendors (e.g., Defender, Sentinel, Palo Alto) and open-source XDR/SIEM systems. \n2. Parse the raw logs, handling any format (`xml`, `json`, etc.) for various vendors and log types. \n3. Dynamically generate `blobl` files according to the parsed information using [LangGraph](https://github.com/langchain-ai/langgraph). \n4. Transform the raw logs into OCSF format using the generated `blobl` files. \nMore details on the concrete implementation of the mapping can be found here: [A data streaming pipeline to convert AWS CloudTrail logs to OCSF using Benthos](https://github.com/ocsf/examples/tree/main/mappings/bloblang/AWS/CloudTrail) It is an example. There are additional mapping pipelines in the repository. We want similar IaC setup for our ETL pipelines.\n##  B. Preference Pair Generation \n1. Convert the OCSF-formatted data into preference pairs. Different types of dataset (e.g., BRON which is a graph database, plaintext, code, queries, binary decompilation, malware analyses etc.) would have different process for this step. Therefore, we could dynamically create preference pairs for a wide range of datasets using [LangGraph](https://github.com/langchain-ai/langgraph). \n2. Create different preference pairs based on hierarchy and levels of abstraction in the dataset. \n##  C. Model Fine-tuning \n1. Start with a pre-trained foundation model (e.g., [Llama 3 405B](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B))\n2. Incorporate the following data sources. The preference pairs for each of the following datasets would have to be organized hierarchically: \n\t* OCSF-formatted logs,\n\t* BRON (as Threat Intel) data,\n\t* Cybersecurity textbooks and domain-specific knowledge, \n\t* Wikipedia pages on networking and cyber security protocols, \n\t* RFC documents, etc.\n3. Use [DSPy](https://dspy-docs.vercel.app) to programmatically fine-tune the model using the corresponding hierarchical preference pairs from above datasets according to [ORPO](https://arxiv.org/abs/2403.07691)\n4. Use BRON data as labels for supervised fine-tuning, showing examples of existing links between attack patterns, tactics etc. \n##  D. Neurosymbolic Architecture \n1. Implement [LogiCOT](https://arxiv.org/pdf/2309.13339) as the primary symbolic component. \n2. Explore the possibility of generating novel attack patterns based on undiscovered edges in the knowledge graph. \n3. Consider incorporating dynamic tool integration for hypotheses validation and other neurosymbolic reasoning enhancement purposes. This system would be similar to how GPT-4 generates and validates Python code using its built-in interpreter.\n\t* Allow the model to generate queries (e.g., for Elastic Search or Splunk) and use the query results to validate its hypotheses. \n\t* The setup could be similar to [the use cases of OCSF using Splunk](https://github.com/ocsf/examples/tree/main/use-cases/splunk), where queries and correlations are performed on the OCSF data using a common SOC tool.\n##  E. Evaluation \n1. Evaluate the model's performance, particularly its ability to correlate OCSF and BRON data.\n2. Assess the model's capability to generate and validate hypotheses using the integrated tools.",
  "slug": "02 product/Building Vendor Agnostic Data Transformation Pipeline for OCSF Schema Conversion",
  "path": "02 product/Building Vendor Agnostic Data Transformation Pipeline for OCSF Schema Conversion.md"
}