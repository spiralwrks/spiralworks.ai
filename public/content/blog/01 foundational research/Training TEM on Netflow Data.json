{
  "title": "Training TEM on Netflow Data",
  "date": "2025-01-12T20:19:15.092Z",
  "tags": [],
  "content": "#fundamental-research\n\n## Key Concepts from the TEM paper\n\n1. **Hippocampal-Entorhinal System**: The system is vital for spatial and relational memory tasks. The paper proposes that medial entorhinal cells form a basis describing structural knowledge, and hippocampal cells link this basis with sensory representations.\n    \n2. **Structural Generalization**: The paper casts spatial and relational inferences as structural generalization. It considers the unsupervised learning problem where an agent must predict the next sensory experience in a sequence derived from probabilistic transitions on graphs.\n    \n3. **Tolman-Eichenbaum Machine (TEM)**: TEM is designed to generalize structural knowledge in space and non-space, predict a broad range of neuronal representations observed in spatial and relational memory tasks, and account for observed remapping phenomena in both the hippocampus and entorhinal cortex.\n    \n4. **Factorization and Conjunction**: The paper emphasizes the separation of structural codes from sensory codes, allowing generalization over environments sharing the same structure. The conjunctive code represents the current environment in the context of this learned structure.\n    \n5. **Path Integration and Relational Memories**: TEM must learn structural codes that represent each state differently so that different memories can be stored and retrieved. Relational memories conjunctively combine the factorized structural and sensory codes, thus storing information about what was where.\n    \n6. **Hierarchical Organization**: When representing tasks that have self-repeating structure, it is efficient to organize cognitive maps hierarchically.\n\n\n<br>\n\n--- \n\n<br>\n\n\n## NF-UQ-NIDS v2 Dataset Graph Representation\n\n- **Nodes**: In the context of the NF-UQ-NIDS v2 dataset, nodes can represent individual network entities such as IP addresses or protocols. They can be defined by features like IP protocol identifier byte, Layer 7 protocol, etc.\n- **Edges**: Edges can represent the connections or flows between these entities. They can be characterized by features like incoming/outgoing bytes and packets, flow duration, TCP flags, etc.\n- **Features**: The 43 features provided in the dataset can be used to define the attributes of the nodes and edges. For example, features related to bytes and packets can describe the edges, while features related to protocols and TTL can describe the nodes.\n<br>\n\n---\n\n<br>\n\n## 5/27/2024 - New Approach\n\n* **Nodes**: Each feature is an observation at a node\n\n* **Edges**: Are actions or transitions that are essentially like time steps in an RNN. They are traversed to find new observations\n\n* **Formal problem definition**: Given observations, $x=\\{x_1,x_2,x_3,...,x_n\\}$ and actions, $a=\\{a_1,a_2,a_3,...,a_n\\}$ the model has to predict what $x_{n+1}$ is for the action $a_{n+1}$\n\n\n<br>\n\n",
  "slug": "01 foundational research/Training TEM on Netflow Data",
  "path": "01 foundational research/Training TEM on Netflow Data.md"
}