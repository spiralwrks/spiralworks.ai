{
  "title": "Why Backprop Sucks",
  "date": "2025-01-12T20:19:15.093Z",
  "tags": [],
  "content": "#fundamental-research \n\n## Sources\n\n- [A Review of Neuroscience-Inspired Machine Learning](https://arxiv.org/html/2403.18929v1)\n\n- [Inferring neural activity before plasticity as a foundation for learning beyond backpropagation](https://www.nature.com/articles/s41593-023-01514-1#Sec2)\n\n<br>\n\n## Credit Assignment\n\n1 of the key tasks in AI is constructing mathematical/algorithmic solutions to the **grand problem of credit assignment**.\n\nWhat is **effective credit assignment**?\n\n(i) identification of which neural processing elements have an influence on a task-specific objective functional $\\mathcal{L}(\\Theta)$.\n\n- NPEs = neural processing elements, individual computations in a computation graph\n\n(ii) modifying synapses that connect all NPEs based on their degree of influence to optimize this objective\n\n- the synaptic adjustments in this step improves the overall performance of the network comprised of the set of NPEs\n\n<br>\n\nWhen we look at credit assignment through the lens of **error-driven learning/adaptation**:\n\n- its carried out by computing + assigning error values to each NPE based on cost $\\mathcal{L}(\\Theta)$\n\n- then we obtain $\\Delta$ from these values: the set of all synapse adjustments to be made in $\\Theta$\n\n- finally, the current values of the ANN's params are updated\n\n<br>\n\nHistorically, we have observed this **error-driven adjustment** experimentally and in theory in biological neuronal networks.\n\nThe problem with how ANNs compute this error and allocate it using **backprogation of errors (backprop)**:\n\n- **THIS IS BIOLOGICALLY IMPLAUSIBLE**\n<br>\n\n**Bio-plausible credit assignment** potentially can be used to develop more robust systems with general, human-like capabilities and faster, energy-efficient training + inference.\n\n- Suitable for neuromorphic hardware implementations because of locality of operations and synaptic updates\n\n- locality here is the opposite of the Von-Neumann architecture where memory and computing units are separate\n\n- when translating locality to hardware, it enables **full parallelization** of operations with low latency, power consumption, and often without supervision\n\n\n**Locality of operations** is the key to enabling the training of networks with entangled and cyclic topologies such as neural circuits **without saving gradients in memory** which is required by backprop through time.\n\n\n```ad-important\n\nMost of current neuroscience-inspired credit assignment research focuses on **energy-based, forward only, or spiking algos that are bio-plausible.**\n\n```\n\n<br>\n\n## What's Wrong with Backprop?\n\nBackprop-based credit assignment's empirical success is very misleading because it's actually **biologically implausible** with several major drawbacks:\n\n- **Weight Transport**: Refers to models using the same weights to perform forward AND backward passes. In NNs using backprop for training, **presynaptic NPEs** receive error gradient info from **postsynaptic NPEs** through the same synaptic connections used originally to forward propagate info\n\t- \n",
  "slug": "01 foundational research/Why Backprop Sucks",
  "path": "01 foundational research/Why Backprop Sucks.md"
}