{
  "title": "A Flexible OCSF ETL Pipeline with Benthos",
  "date": "2026-05-12T00:08:42.642Z",
  "content": "# Introduction\n\nThis document outlines the development process of setting up a Benthos pipeline aka Bento for processing raw data from MinIO, transforming it using BLOBL mappings, and outputting to OpenSearch or an error bucket. We'll explore the key components and how they work together to create a flexible and error-resistant data processing pipeline.\n\n# Pipeline Overview\n\nThe pipeline consists of the following main steps:\n\n1. Input: Raw data ingestion from MinIO buckets\n2. Processing: Data transformation using BLOBL mappings\n3. Output: Sending processed data to OpenSearch or error data to a MinIO bucket\n\n# Key Components\n\n# 1. Input Configuration (input_bucket.yml)\n\nThe `input_bucket.yml` file defines how Bento ingests data from MinIO buckets:\n```yaml\ninput_resources:\n    - label: input_bucket\n      broker:\n        copies: 1\n        inputs:\n          - aws_s3:\n              bucket: \"minio\"\n              region: \"us-east-1\"\n              endpoint: \"http://:9000/aws\"\n              credentials:\n                id: \"admin\"\n                secret: \"password\"\n              delete_objects: true\n              scanner:\n                json_documents: {}\n            processors:\n                  - mapping: |\n                      root = this.Records\n                  - unarchive:\n                      format: json_array\n          \n          - aws_s3:\n              bucket: \"minio\"\n              region: \"us-east-1\"\n              endpoint: \"http://:9000/okta\"\n              credentials:\n                id: \"admin\"\n                secret: \"password\"\n              delete_objects: true\n              scanner:\n                json_documents: {}     \n```\nThis configuration sets up two S3 inputs, one for AWS logs and another for Okta logs. It specifies the MinIO bucket, region, endpoint, and credentials for each input. The `processors` section for the AWS input includes a mapping step to extract records and an unarchive step to handle JSON arrays.\n\n# 2. Processing Configuration (bento.yml)\n\nThe `bento.yml` file defines the overall pipeline structure:\n```yaml\ninput:\n  resource: input_bucket\n\npipeline:\n  processors:\n    - resource: pre_processor\n    - resource: processor_ocsf\n    - resource: processor_error_log\n\noutput:\n  resource: output_bucket\n  processors:\n     - resource: output_error_log\n\nlogger:\n  level: INFO\n  format: json\n  add_timestamp: false\n  static_fields:\n    '@service': bento\n```\nThis configuration specifies the input resource, processing steps, and output resource. It also includes error logging for both processing and output stages.\n\n# 3. Output Configuration (output_bucket.yml)\n\nThe `output_bucket.yml` file defines how processed data is sent to OpenSearch or the error bucket:\n```yaml\noutput_resources:\n  - label: output_bucket\n    switch:\n      cases:\n        - check: '@.exists(\"ocsf_bucket\")'\n          output:\n            broker:\n              outputs:\n                - opensearch:\n                    urls: [http://opensearch:9200]\n                    index: 'ocsf-${!this.class_uid}-${! meta(\"ocsf_bucket\")}'\n                    action: \"index\"\n                    id: \"\"\n        - output:       \n            aws_s3:\n              bucket: \"minio\"\n              path: error-${!timestamp_unix_nano()}.json\n              region: \"us-east-1\"\n              endpoint: \"http://:9000/ocsf-error\"\n              credentials:\n                id: \"admin\"\n                secret: \"password\"\n```\nThis configuration uses a switch statement to determine the output destination:\n\n- If the `ocsf_bucket` metadata exists, the data is sent to OpenSearch.\n- Otherwise, it's considered an error and sent to the MinIO error bucket.\n\n# 4. Error Handling\n\nTwo error handling components are defined:\n\n#### `processor_error_log.yml`\n```yaml\nprocessor_resources:\n  - label: processor_error_log\n    catch:\n      - metric:\n          type: counter\n          name: failed_transform\n          labels:\n            transform: ${! meta(\"transform_name\") }\n      - log:\n          level: ERROR\n          message: \"Transform failed\"\n          fields_mapping: | \n            root.reason = error()\n            root.topic = meta(\"ocsf_bucket\")\n            root.transform_name = meta(\"transform_name\")\n            root.raw_message = this\n```\nThis handles errors during the processing stage, logging details about the failed transformation.\n\n#### `output_error_log.yml`\n```yaml\nprocessor_resources:\n  - label: output_error_log\n    catch:\n      - metric:\n          type: counter\n          name: failed_output\n          labels: \n            transform: ${! meta(\"transform_name\") }       \n      - log:\n          level: ERROR\n          message: \"Output failed: \"\n          fields_mapping: | \n            root.reason = error()            \n            root.ocsf_bucket = meta(\"ocsf_bucket\")               \n            root.transform_name = meta(\"transform_name\")\n```\n# Development Process\n\n1. **Input Configuration**: Set up the `input_bucket.yml` to correctly ingest data from MinIO buckets, ensuring proper credentials and endpoints.\n    \n2. **Processing Configuration**: Configure the `bento.yml` to define the pipeline structure, including pre-processing, OCSF processing, and error handling.\n    \n3. **Output Configuration**: Set up the `output_bucket.yml` to direct processed data to OpenSearch and error data to the MinIO error bucket.\n    \n4. **Error Handling**: Implement error logging for both processing and output stages to capture and log any issues during data transformation or output.\n    \n5. **BLOBL Mappings**: Create BLOBL mapping files for different log types (e.g., `AWS CloudTrail`, `Okta`) to transform raw data into the desired OCSF format.\n    \n6. **Testing and Debugging**: Iteratively test the pipeline with sample data, adjusting configurations and mappings as needed. Pay special attention to endpoint accessibility and error handling.\n    \n7. **Optimization**: Fine-tune the pipeline for performance, adjusting batch sizes, processing steps, and error handling as necessary.\n\nThe resulting Bento pipeline provides a flexible and robust solution for processing raw log data from various sources. It automatically collects data from MinIO, processes it using BLOBL mappings, and outputs the results to OpenSearch. Any data that cannot be mapped is redirected to an error bucket in MinIO for further analysis. The modular design and comprehensive error handling make this pipeline adaptable to various data processing needs and resilient to potential issues.",
  "category": "02 product",
  "tags": [],
  "slug": "a-flexible-ocsf-etl-pipeline-with-benthos",
  "path": "02 product/A Flexible OCSF ETL Pipeline with Benthos"
}