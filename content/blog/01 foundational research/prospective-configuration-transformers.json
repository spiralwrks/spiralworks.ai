{
  "title": "Prospective Configuration Transformers",
  "date": "2026-05-12T00:08:42.633Z",
  "content": "#fundamental-research \n\n## PC-Transformer (Standard Prospective Configuration)\n\n<br>\n\n#### Energy-Based Attention\n\nInstead of using dot-product attention, we'll define an energy function for attention:\n\n$E_{att}(Q, K, V) = \\sum_{i,j} (A_{ij} - softmax(\\frac{QK^T}{\\sqrt{d_k}})_{ij})^2$\n\nwhere $A$ is the attention matrix we're trying to learn.\n\n<br>\n\n#### Prospective Configuration for Attention \n\nRelaxation phase: $$\\Delta A = -\\gamma \\frac{\\partial E_{att}}{\\partial A}$$\n\nWeight update phase: $$\\Delta W_Q = -\\alpha \\frac{\\partial E_{att}}{\\partial W_Q}|_{A=A^*}$ $\\Delta W_K = -\\alpha \\frac{\\partial E_{att}}{\\partial W_K}|_{A=A^*}$ $\\Delta W_V = -\\alpha \\frac{\\partial E_{att}}{\\partial W_V}|_{A=A^*}$$\nwhere $A^*$ is the converged attention matrix after relaxation.\n\n<br>\n\n#### Multi-Head Prospective Attention (MHPA)\n\nFor each head $i$:\n\n1. Compute initial $Q_i, K_i, V_i$\n2. Perform relaxation on $A_i$ to minimize $E_{att}(Q_i, K_i, V_i)$\n3. Update weights using converged $A_i^*$\n\nThe output of MHPA is: $MHPA(Q, K, V) = Concat(head_1, ..., head_h)W^O$\n\nwhere $head_i = A_i^*V_i$\n\n<br>\n\n#### Position-wise Energy Network (PEN)\n\nReplace the feed-forward network with an energy-based network:\n\n$E_{PEN}(x) = |x - FFN(x)|^2 + \\lambda \\cdot R(x)$\n\nwhere $FFN$ is a standard feed-forward network and $R(x)$ is a regularization term.\n\n<br>\n\n#### Layer Architecture\n\nEach layer in the PC-Transformer consists of:\n\n1. Multi-Head Prospective Attention (MHPA)\n2. Layer Normalization\n3. Position-wise Energy Network (PEN)\n4. Layer Normalization\n5. Full PC-Transformer\n\nThe full PC-Transformer processes input $x$ as follows:\n\na) Embedding + Positional Encoding: $h_0 = Emb(x) + PE(x)$\n\nb) For each layer $l=1...L$: $h_l' = LN(MHPA(h_{l-1}) + h_{l-1})$ $h_l = LN(PEN(h_l') + h_l')$\n\nc) Output: $y = LN(h_L)$\n\n<br>\n\n#### Training Procedure\n\nFor each training example:\n\n1. Forward pass through PC-Transformer\n2. Compute total energy: $E_{total} = E_{task} + \\sum_l (E_{att,l} + E_{PEN,l})$ where $E_{task}$ is task-specific (e.g., cross-entropy for classification)\n3. Relaxation phase: Update all dynamic variables (attention matrices, PEN activations) to minimize $E_{total}$\n4. Weight update phase: Update all weights using converged values\n5. Inference\n\nDuring inference, perform steps 1-3 of the training procedure, then read out the final layer's output.\n\n<br>\n\n---\n\n<br>\n\n## IPC-Transformer (Incremental Prospective Configuration)\n\n<br>\n\n## Overview\n\nThe IPC-Transformer combines the transformer architecture with principles from Incremental Predictive Coding (IPC) and Prospective Configuration (PC). It emphasizes local computations, simultaneous updates of activities and weights, and iterative refinement of predictions.\n<br>\n\n## Layer-Wise Local Energy Functions\n\nEach layer $l$ has its own local energy function:\n$$\nE_l = \\frac{1}{2} \\left\\lVert\\varepsilon^{(l)}\\right\\rVert^2 = \\frac{1}{2} \\left\\lVert x^{(l)} - \\mu^{(l)}\\right\\rVert^2\n$$\nWhere:\n- $\\varepsilon^{(l)}$ is the prediction error at layer $l$\n- $x^{(l)}$ is the actual activity at layer $l$\n- $\\mu^{(l)}$ is the predicted activity for layer $l$\n\n<br>\n\n\n",
  "category": "01 foundational research",
  "tags": [],
  "slug": "prospective-configuration-transformers",
  "path": "01 foundational research/Prospective Configuration Transformers"
}